# Latent knowledge in the Paleocene Eocene boundary research

---

#### An exploration into using unsupervised learning methods to pull out the themes or topics of a large body of scientific text.  


Global warming, attributed to increasing carbon dioxide in the atmosphere, causes climate shifts and sea-level rises that have great social and economic impacts on populations.  

Some 56 million years ago, at the Paleocene-Eocene boundary, the world ran a similar experiment.  
Geological evidence indicates a 4â€“6 C temperature increase, that lasted about 200,000 years. Precipitation patterns changed dramatically, with associated changes in worldwide terrestrial plant distribution. Sea levels rose by several meters, inundating shallow coastal belts. Some marine plants and animals became extinct while others had population explosions. 

Geologists have studied these ancient effects to better understand what is happening today. 
One example is here in Bastrop County outside of Austin, TX.  Resulting publications that directly address this topic are now covering a wide range of data types, from sediments to fossils to carbon isotopes, but with uneven worldwide coverage. 
 
An evaluation and summary of this data is currently being undertaken to provide a snapshot of current knowledge and to suggest future research directions.   With the amount of information involved there is the risk of under-emphasizing seminal publications, over-emphasizing peripheral publications, seeing patterns where none exist, and missing links where they do exist. 



__Resources Used:__
- Data Collection and Cleaning
  - pyPDF2
  - Gensim
  - text cleaning
- Unsupervised Learning
  - NLP
  - Topic Modeling
  - Word2Vec

__Python Packages Used:__
- Pandas
- sklearn
- gensim
- NLTK
- regex
- pyLDAvis

---

## Objective

_Problem Statement_

__Using Natural Language Processing and Machine Learning, what can we learn about latent knowledge from Paleocene Eocene boundary research and can this give us future avenues to explore?__


## Data


_Data Collection_

__1. Find source for PETM research__
  - The data source was generated out of documents collected over 30 years by Christoper Denison PhD. The issue lies in that a preponderence of scientific knowledge is in the form of text, housed in journals and publications.  In the last few years research has now been able to be read online and downloaded with a fee.  The pdf downloads are not consistent and most researchers turn to scanning their old journals. 

__2. Turn the pdfs of research into usable text__
  - To generate a text file of the pdf's, I used the package pyPDF2 to read pdf's individually and save them, again individually into their own text files. 

```python
pdfFiles = []

for filename in glob.glob('./data/input_pdfs/**/*.pdf', recursive = True):  
    if filename.endswith('.pdf'):   
        pdfFiles.append(filename) 
        
for filename in pdfFiles:
   
    try:
        f =  open(f'{filename}', 'rb') 
        f2 = open(f'./data/output_txt/{filename.rsplit("/",1)[-1][:-4]}.txt', 'w') 
        pdf = PdfFileReader(f)
        print(f'reading {filename}')

        for pageNum in range(0,pdf.numPages):
            page = pdf.getPage(pageNum)
            text = page.extractText()
            f2.write(text)
            #print(f'page{pageNum}')
    except:
        print (f"busted file {filename}")
```

__3. Data Cleaning__
  - Once each pdf had been read and saved as an individual text file it was necessary to clean up the text a bit. I chose to use the preprocessing module and example from gensim documentation to strip out punctuation, numbers, short words, multiple whitespaces and english stopwords.   I also removed 115 corpus specific stop words.   These were journal names, research places, publisher names and image data.  I chose to leave in the reference section of each document.  
  
  ```python
from gensim import utils
import gensim.parsing.preprocessing as gsp

filters = [
    gsp.strip_tags,     
    gsp.strip_punctuation,
    gsp.strip_numeric,
    gsp.remove_stopwords, 
    gsp.strip_short, 
    gsp.stem_text,
    gsp.strip_multiple_whitespaces 
          ]

def clean_text(s):
    s = s.lower()
    s = utils.to_unicode(s)
    for f in filters:
        s = f(s)
    return s
```

  -  There were a total of 2031 documents out of over 10,000 that eventually were usable in modeling. It turned out that 80% of the pdf's had read in as blank text files.   Most were due to the way the document had been orginally scanned into a pdf.   Some were screen shots from the internet and some were just unreadable due to age, such as one small book from 1860.    



## Models

### Topic Modeling

_Model the Data using Unsupervised Learning_

To conduct topic modeling, Latent Dirichlet Allocation (LDA) was run from two different python libraries.  
  - sklearn
  - gensim
  
Aim of using the LDA method is to exhibit latent topics, which are distributions of word frequency, among all documents

Each model type was fed the TFIDF vectorizer results. At first, the  decision on how many number of topics to use was difficult to isolate on where to draw the line. To help this, I performed a gridsearch in sklearn and used the coherence and perplexity scores to map how well the topics were being clustered in relevant groups.


```python
from gensim.models.coherencemodel import CoherenceModel

coherence_model_lda = CoherenceModel(model=lda_model,
                                     texts=texts,
                                     dictionary=dictionary,
                                     coherence='c_v')

coherence_lda = coherence_model_lda.get_coherence()
print('Coherence Score: ', coherence_lda)   # higher is better
print('Perplexity:', lda_model.log_perplexity(corpus))  #lower is better
```


_Evaluate the Data_

Topics that were able to be isolated:
- Topic 1: ages 
- Topic 2: location
- Topic 3: sedimentology - broad
- Topic 4: sequence stratigraphy
- Topic 5: ages
- Topic 6: unknown pattern
- Topic 7: detailed sedimentology
- Topic 8: authors



## Futurework

There is need to continue this work for the geology/sedimentology scientific research.  What we can discover can help catapult the discovery of latent knowledge in the research.  In the future this project will encorporate the following. 

- PDF reader debugging or find a new reader that can handle a wider range of pdf images and translate into text.  
- Train / test using prior work to train models and the lastest work to test and model for predictions
- Utilize Word2Vec and Doc2Vec more thoroughly to pull out words and associated papers that look unique in the topic environments. 


## Conclusion

_Answer the Problem_

Topics were able to be assigned to each document and showed a distinct and unique flow.  This does not indicate there were any gaps associated in research at this time.  Training a larger vocabulary associated with PETM research can improve these results.   


