{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import  TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Read in dataframe of text and filenames\n",
    "df=pd.read_pickle('assets/df_clean2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>filename</th>\n",
       "      <th>year</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proceed yorkshir geolog societi vol upper jura...</td>\n",
       "      <td>Cox et al 1987</td>\n",
       "      <td>1987</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>triassicpalynologyofcentralandnorthwesterneuro...</td>\n",
       "      <td>Kuerschner &amp; Herngreen 2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>journalofsedimentaryresearch currentrippl doi ...</td>\n",
       "      <td>Gani 2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ˆˆˆ˙ ˛kˆ c˝ˇ hh˛ hhˇ lk˘ hhˇ hhˇ˚ hhˇ d˜˛ hhˇ ...</td>\n",
       "      <td>Iakovleva Brinkhuis &amp; Cavagnetto 2001</td>\n",
       "      <td>2001</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>field excurs novemb tertiari format austin hou...</td>\n",
       "      <td>Wilson 1962</td>\n",
       "      <td>1962</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  proceed yorkshir geolog societi vol upper jura...   \n",
       "1  triassicpalynologyofcentralandnorthwesterneuro...   \n",
       "2  journalofsedimentaryresearch currentrippl doi ...   \n",
       "3  ˆˆˆ˙ ˛kˆ c˝ˇ hh˛ hhˇ lk˘ hhˇ hhˇ˚ hhˇ d˜˛ hhˇ ...   \n",
       "4  field excurs novemb tertiari format austin hou...   \n",
       "\n",
       "                                filename  year  split  \n",
       "0                         Cox et al 1987  1987   test  \n",
       "1            Kuerschner & Herngreen 2010  2010  train  \n",
       "2                              Gani 2017  2017  train  \n",
       "3  Iakovleva Brinkhuis & Cavagnetto 2001  2001  train  \n",
       "4                            Wilson 1962  1962   test  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus specific stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific = ['figs', 'fig', 'et', 'al', 'pl','appendix','figure','cm', 'ft', 'sp'\\\n",
    "            , 'pp', 'iv', 'etal', 'ed', 'eds', 'http', 'ma', 'th', 'tion', 'ing',\\\n",
    "           'cf', 'ii', 'www', 'tions', 'strati', 'km', 'com', 'bulletin', 'doi', \\\n",
    "            'org', 'society','springer', 'verlag', 'pa', 'spec', 'pub', 'assoc',\\\n",
    "            'publication','university', 'press', 'geologists', 'geological',\\\n",
    "            'association', 'ph', 'comm', 'pers', 'geol', 'surv', 'bull',\\\n",
    "            'journal', 'soc', 'sci', 'letters', 'lett', 'geophys', 'res',\\\n",
    "            'acad', 'mar', 'acad', 'palaeobotany', 'palaeoclimatology', \\\n",
    "            'palaeogeography','societies', 'bureau', 'economic', 'prof',\\\n",
    "            'palaeoecology','paper', 'file', 'report', 'open', 'london',\\\n",
    "            'america', 'elsevier','amsterdam', 'sepm', 'earthplanet',\\\n",
    "           'paleoclimatol', 'palaeoecol', 'np', 'sc', 'palaeogeogr', 'palaeoclimatol',\\\n",
    "            'american', 'geo', 'rev', 'journal', 'und', 'review', 'samples',\\\n",
    "            'collected', 'allrightsreserved', 'clim', 'elsevierb', 'cosmochim',\\\n",
    "            'sciencereviews', 'levelchanges', 'ne', 'sepmspec', 'publ', 'acta',\\\n",
    "           'internationalassociationofsedimentologists', 'palaeobot', 'polynol',\\\n",
    "           'sedi', 'ment', 'deposi', 'tional', 'odp']\n",
    "\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the sparse matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (2,2),\n",
    "                             #stop_words = specific + stop_words,\n",
    "                             max_features = 200\n",
    "                             )\n",
    "sparse = vectorizer.fit_transform(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1830, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alberta canada</th>\n",
       "      <th>american associ</th>\n",
       "      <th>associ geolog</th>\n",
       "      <th>associ petroleum</th>\n",
       "      <th>barrier island</th>\n",
       "      <th>base level</th>\n",
       "      <th>bed form</th>\n",
       "      <th>benthic foraminifer</th>\n",
       "      <th>benthic foraminifera</th>\n",
       "      <th>bound ari</th>\n",
       "      <th>...</th>\n",
       "      <th>trough cross</th>\n",
       "      <th>unit state</th>\n",
       "      <th>univers texa</th>\n",
       "      <th>upper cretac</th>\n",
       "      <th>upper wilcox</th>\n",
       "      <th>van wagon</th>\n",
       "      <th>volcan ash</th>\n",
       "      <th>water depth</th>\n",
       "      <th>wave domin</th>\n",
       "      <th>wilcox group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05526</td>\n",
       "      <td>0.068325</td>\n",
       "      <td>0.305531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alberta canada  american associ  associ geolog  associ petroleum  \\\n",
       "0             0.0              0.0            0.0               0.0   \n",
       "1             0.0              0.0            0.0               0.0   \n",
       "2             0.0              0.0            0.0               0.0   \n",
       "3             0.0              0.0            0.0               0.0   \n",
       "4             0.0              0.0            0.0               0.0   \n",
       "\n",
       "   barrier island  base level  bed form  benthic foraminifer  \\\n",
       "0             0.0         0.0       0.0                  0.0   \n",
       "1             0.0         0.0       0.0                  0.0   \n",
       "2             0.0         0.0       0.0                  0.0   \n",
       "3             0.0         0.0       0.0                  0.0   \n",
       "4             0.0         0.0       0.0                  0.0   \n",
       "\n",
       "   benthic foraminifera  bound ari  ...  trough cross  unit state  \\\n",
       "0                   0.0        0.0  ...           0.0     0.00000   \n",
       "1                   0.0        0.0  ...           0.0     0.00000   \n",
       "2                   0.0        0.0  ...           0.0     0.00000   \n",
       "3                   0.0        0.0  ...           0.0     0.00000   \n",
       "4                   0.0        0.0  ...           0.0     0.05526   \n",
       "\n",
       "   univers texa  upper cretac  upper wilcox  van wagon  volcan ash  \\\n",
       "0      0.000000      0.000000           0.0        0.0         0.0   \n",
       "1      0.000000      0.000000           0.0        0.0         0.0   \n",
       "2      0.000000      0.000000           0.0        0.0         0.0   \n",
       "3      0.000000      0.000000           0.0        0.0         0.0   \n",
       "4      0.068325      0.305531           0.0        0.0         0.0   \n",
       "\n",
       "   water depth  wave domin  wilcox group  \n",
       "0          0.0         0.0      0.000000  \n",
       "1          0.0         0.0      0.000000  \n",
       "2          0.0         0.0      0.000000  \n",
       "3          0.0         0.0      0.000000  \n",
       "4          0.0         0.0      0.136959  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# densifying\n",
    "df_vec = pd.DataFrame(sparse.todense(), \n",
    "                  columns=vectorizer.get_feature_names())\n",
    "df_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vec.to_pickle('assets/df_vec.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity - dot product of normalized vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04248589, 0.        , ..., 0.        , 0.        ,\n",
       "        0.01573449],\n",
       "       [0.04248589, 1.        , 0.07232913, ..., 0.        , 0.        ,\n",
       "        0.04880497],\n",
       "       [0.        , 0.07232913, 1.        , ..., 0.        , 0.        ,\n",
       "        0.00886777],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.00286516],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.04062185],\n",
       "       [0.01573449, 0.04880497, 0.00886777, ..., 0.00286516, 0.04062185,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos = cosine_similarity(df_vec)\n",
    "cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 8\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=num_topics, max_iter=50,\n",
    "                                verbose=1, evaluate_every=10,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=60.,\n",
    "                                learning_decay = .5,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 50\n",
      "iteration: 2 of max_iter: 50\n",
      "iteration: 3 of max_iter: 50\n",
      "iteration: 4 of max_iter: 50\n",
      "iteration: 5 of max_iter: 50\n",
      "iteration: 6 of max_iter: 50\n",
      "iteration: 7 of max_iter: 50\n",
      "iteration: 8 of max_iter: 50\n",
      "iteration: 9 of max_iter: 50\n",
      "iteration: 10 of max_iter: 50, perplexity: 301.3808\n",
      "iteration: 11 of max_iter: 50\n",
      "iteration: 12 of max_iter: 50\n",
      "iteration: 13 of max_iter: 50\n",
      "iteration: 14 of max_iter: 50\n",
      "iteration: 15 of max_iter: 50\n",
      "iteration: 16 of max_iter: 50\n",
      "iteration: 17 of max_iter: 50\n",
      "iteration: 18 of max_iter: 50\n",
      "iteration: 19 of max_iter: 50\n",
      "iteration: 20 of max_iter: 50, perplexity: 300.7195\n",
      "iteration: 21 of max_iter: 50\n",
      "iteration: 22 of max_iter: 50\n",
      "iteration: 23 of max_iter: 50\n",
      "iteration: 24 of max_iter: 50\n",
      "iteration: 25 of max_iter: 50\n",
      "iteration: 26 of max_iter: 50\n",
      "iteration: 27 of max_iter: 50\n",
      "iteration: 28 of max_iter: 50\n",
      "iteration: 29 of max_iter: 50\n",
      "iteration: 30 of max_iter: 50, perplexity: 300.4452\n",
      "iteration: 31 of max_iter: 50\n",
      "iteration: 32 of max_iter: 50\n",
      "iteration: 33 of max_iter: 50\n",
      "iteration: 34 of max_iter: 50\n",
      "iteration: 35 of max_iter: 50\n",
      "iteration: 36 of max_iter: 50\n",
      "iteration: 37 of max_iter: 50\n",
      "iteration: 38 of max_iter: 50\n",
      "iteration: 39 of max_iter: 50\n",
      "iteration: 40 of max_iter: 50, perplexity: 300.3036\n",
      "iteration: 41 of max_iter: 50\n",
      "iteration: 42 of max_iter: 50\n",
      "iteration: 43 of max_iter: 50\n",
      "iteration: 44 of max_iter: 50\n",
      "iteration: 45 of max_iter: 50\n",
      "iteration: 46 of max_iter: 50\n",
      "iteration: 47 of max_iter: 50\n",
      "iteration: 48 of max_iter: 50\n",
      "iteration: 49 of max_iter: 50\n",
      "iteration: 50 of max_iter: 50, perplexity: 300.1671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03753319, 0.03747185, 0.03747173, ..., 0.0375102 , 0.73749488,\n",
       "        0.03753212],\n",
       "       [0.04465579, 0.04465606, 0.0446558 , ..., 0.04465896, 0.25636096,\n",
       "        0.04469977],\n",
       "       [0.03836167, 0.03835891, 0.03854356, ..., 0.0383957 , 0.03836012,\n",
       "        0.03836795],\n",
       "       ...,\n",
       "       [0.03188351, 0.77628236, 0.03198317, ..., 0.03199058, 0.03195093,\n",
       "        0.03210358],\n",
       "       [0.05553317, 0.46349631, 0.05538076, ..., 0.05539675, 0.20403734,\n",
       "        0.05538752],\n",
       "       [0.0281114 , 0.02804955, 0.02805816, ..., 0.02833781, 0.80321044,\n",
       "        0.02811703]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit_transform(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Likelihood:  -25056.920617093936\n",
      "Perplexity:  300.16708960714647\n"
     ]
    }
   ],
   "source": [
    "print(\"Log Likelihood: \", lda.score(sparse)) #higher the better\n",
    "print(\"Perplexity: \", lda.perplexity(sparse)) #Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "\n",
    "#perplexity might not be the best measure to evaluate topic models because it doesn’t consider the context and semantic associations between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch LDA using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    'n_topics': [2,3,4,5], \n",
    "    'learning_decay': [.5,.6, .7, .8], \n",
    "    'learning_offset': [60.,70., 80.]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_topics': [2, 3, 4, 5], 'learning_decay': [0.5, 0.6, 0.7, 0.8], 'learning_offset': [60.0, 70.0, 80.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_grid = LatentDirichletAllocation(max_iter=10, learning_method='online',random_state=42)\n",
    "\n",
    "grid_model = GridSearchCV(lda_grid, param_grid=search_params)\n",
    "grid_model.fit(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7973.164979311252\n",
      "{'learning_decay': 0.5, 'learning_offset': 60.0, 'n_topics': 2}\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
